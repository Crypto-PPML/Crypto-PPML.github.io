{
  "name": "CRYPTO-PPML 2021",
  "config": {
    "default_track_locations": [
      {
        "name": "Enter a location"
      }
    ],
    "default_talk_minutes": 25,
    "unassigned_talks": [
      {
        "name": "Invited Talks",
        "talks": [],
        "id": "category-1"
      },
      {
        "name": "Uncategorized",
        "talks": [],
        "id": "category-0"
      }
    ],
    "uniqueIDIndex": 31,
    "timezone": {
      "name": "UTC",
      "abbr": "UTC"
    }
  },
  "days": [
    {
      "date": "2021-08-15",
      "timeslots": [
        {
          "endtime": "14:50",
          "starttime": "14:00",
          "sessions": [
            {
              "session_title": "Welcome and Invited Talk",
              "id": "session-2",
              "talks": [
                {
                  "id": "talk-11",
                  "title": "New Techniques for Efficient Secure Computation",
                  "authors": [
                    "Yuval Ishai"
                  ],
                  "affiliations": "Technion",
                  "starttime": "14:00",
                  "endtime": "14:50"
                }
              ]
            }
          ]
        },
        {
          "endtime": "15:40",
          "starttime": "14:50",
          "sessions": [
            {
              "moderator": "Chair: TBD",
              "session_title": "Contributed Talks",
              "talks": [
                {
                  "paperId": 103,
                  "title": "Secure Quantized Training for Deep Learning",
                  "affiliations": "CSIRO's Data61",
                  "abstract": "We have implemented training of neural networks in secure multi-party\r\ncomputation (MPC) using quantization commonly used in the said\r\nsetting. To the best of our knowledge, we are the first to present an\r\nMNIST classifier purely trained in MPC that comes within 0.2 percent\r\nof the accuracy of the same convolutional neural network trained via\r\nplaintext computation. More concretely, we have trained a network with\r\ntwo convolution and two dense layers to 99.2% accuracy in 25\r\nepochs. This took 3.5 hours in our MPC implementation (under one hour\r\nfor 99% accuracy).\r\n\r\nThe talk would be given by Marcel.",
                  "authors": [
                    "Marcel Keller",
                    "Ke Sun"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-23"
                },
                {
                  "paperId": 112,
                  "title": "ABY2.0: Improved Mixed-Protocol Secure Two-Party Computation with Applications to Privacy Preserving Machine Learning",
                  "affiliations": "Indian Institute of Science; TU Darmstadt",
                  "abstract": "Secure Multi-party Computation (MPC) allows a set of mutually distrusting parties to jointly evaluate a function on their private inputs while maintaining input privacy. In this work, we improve semi-honest secure two-party computation (2PC) over rings, with a focus on the efficiency of the online phase. We propose an efficient mixed-protocol framework by extending our techniques to multi-input multiplication gates without inflating the online communication, i.e., it remains independent of the fan-in. Along the way, we construct efficient protocols for several PPML primitives such as scalar product, matrix multiplication, comparison, and maxpool. The  online communication of our scalar product is two ring elements irrespective of the vector dimension, which is a feature achieved for the first time in the Privacy-preserving Machine Learning (PPML) literature.\r\n\r\nThe practicality of our new set of protocols is showcased with Biometric Matching and Privacy-preserving Machine Learning (PPML). Most notably, for PPML, we implement and benchmark training and inference of Logistic Regression and Neural Networks over LAN and WAN networks. For training, we improve online runtime (both for LAN and WAN) over SecureML (Mohassel et al., IEEE S\\&P'17) in the range 1.5x--6.1x, while for inference, the improvements are in the range of 2.5x--754.3x.",
                  "authors": [
                    "Arpita Patra",
                    "Thomas Schneider",
                    "Ajith Suresh",
                    "Hossein Yalame"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-27"
                },
                {
                  "paperId": 106,
                  "title": "SIRNN: A Math Library for Secure RNN Inference",
                  "affiliations": "Microsoft Research, UC Berkeley; Microsoft Research",
                  "abstract": "Complex machine learning (ML) inference algorithms like recurrent neural networks (RNNs) use standard functions from math libraries like exponentiation, sigmoid, tanh, and reciprocal of square root. Although prior work on secure 2-party inference provides specialized protocols for convolutional neural networks (CNNs), existing secure implementations of these math operators rely on generic 2-party computation (2PC) protocols that suffer from high communication.\r\nWe provide new specialized 2PC protocols for math functions that crucially rely on lookup-tables and mixed-bitwidths to address this performance overhead; our protocols for math functions communicate up to 423x less data than prior work.\r\nFurthermore, our math implementations are numerically precise, which ensures that the secure implementations preserve model accuracy of cleartext. \r\nWe build on top of our novel protocols to build SIRNN, a library for end-to-end secure 2-party DNN inference, that provides the first secure implementations of an RNN operating on time series sensor data, an RNN operating on speech data, and a state-of-the-art ML architecture that combines CNNs and RNNs for identifying all heads present in images. Our evaluation\r\nshows that SIRNN achieves up to three orders of magnitude of performance improvement when compared to inference of these models using an existing state-of-the-art 2PC framework.",
                  "authors": [
                    "Deevashwer Rathee",
                    "Mayank Rathee",
                    "Rahul Kranti Kiran Goli",
                    "Divya Gupta",
                    "Rahul Sharma",
                    "Nishanth Chandran",
                    "Aseem Rastogi"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-25"
                },
                {
                  "paperId": 109,
                  "title": "Differential Privacy for Text Analytics via Natural Text Sanitization",
                  "affiliations": "The Ohio State University; The Chinese University of Hong Kong; Carnegie Mellon University; Alibaba Group",
                  "abstract": "Texts convey sophisticated knowledge. However, texts also convey sensitive information. Despite the success of general-purpose language models and domain-specific mechanisms with differential privacy (DP), existing text sanitization mechanisms still provide low utility, as cursed by the high-dimensional text representation. The companion issue of utilizing sanitized texts for downstream analytics is also under-explored. This paper takes a direct approach to text sanitization. Our insight is to consider both sensitivity and similarity via our new local DP notion. The sanitized texts also contribute to our sanitization-aware pretraining and fine-tuning, enabling privacy-preserving natural language processing over the BERT language model with promising utility. Surprisingly, the high utility does not boost up the success rate of inference attacks.\r\n\r\nSherman Chow will be presenting.",
                  "authors": [
                    "Xiang Yue",
                    "Minxin Du",
                    "Tianhao Wang",
                    "Yaliang Li",
                    "Huan Sun",
                    "Sherman S. M. Chow"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-26"
                },
                {
                  "paperId": 102,
                  "title": "Fighting COVID-19 in the Dark: End-to-End Methodology for Improved Inference Using Homomorphically Encrypted DNN",
                  "affiliations": "IBM Research, Bar Ilan University; IBM Research",
                  "abstract": "The ability to run deep neural networks (DNN) while adhering to privacy regulations in untrusted cloud environments is becoming quite critical for various industries such as healthcare, finance, and retail. One interesting approach for secure prediction is Homomorphic Encryption (HE), which enables secure predictions over encrypted data. \r\nHowever, HE comes with a significant computation and communication overhead that limits its adoption in practical applications.\r\n  \r\nIn this paper we consider the CKKS HE scheme because of its efficiency in arithmetic computations. Running AI applications over this scheme is a prolific field of research, and this work leverages recent advancements in usability and speed.\r\nBecause CKKS supports only addition and multiplication,  implementing non-polynomial operators such as ReLU is challenging.\r\nAs a result, transforming a DNN to a CKKS-compatible model that can run over this scheme with minimal accuracy degradation is not a trivial task.\r\n \r\n\r\nIn this work we present a structured methodology to train an efficient CKKS-compatible model based on a pretrained model, with minimal degradation in accuracy, using new techniques for this task such as trainable activation functions and Knowledge Distillation. \r\nIn this work we present a structured methodology to train an efficient CKKS-compatible model based on a pretrained model, with minimal degradation in accuracy, using new techniques for this task such as trainable activation functions and Knowledge Distillation. \r\n As a case study, we chose the task of detecting COVID-19 in both chest X-ray and CT datasets as well as the classical CIFAR10. We evaluated the proposed methodology on AlexNet and Resnet18 by showing trade-offs between efficiency and accuracy of the generated models.",
                  "authors": [
                    "Moran Baruch",
                    "Lev Greenberg",
                    "Guy Moshowich"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-22"
                }
              ],
              "id": "session-8"
            }
          ]
        },
        {
          "endtime": "16:00",
          "starttime": "15:40",
          "sessions": [
            {
              "session_title": "Break",
              "id": "session-1"
            }
          ]
        },
        {
          "endtime": "17:40",
          "starttime": "16:00",
          "sessions": [
            {
              "session_title": "Invited Talks",
              "talks": [
                {
                  "id": "talk-15",
                  "title": "Privacy in Federated Learning at Scale ",
                  "authors": [
                    "Adria Gascon, Peter Kairouz, Kallista (Kaylee) Bonawitz"
                  ],
                  "affiliations": "Google"
                },
                {
                  "id": "talk-14",
                  "title": "TBD",
                  "authors": [
                    "Vinod Vaikuntanathan"
                  ],
                  "affiliations": "MIT"
                }
              ],
              "id": "session-10"
            }
          ]
        },
        {
          "starttime": "17:40",
          "endtime": "18:00",
          "sessions": [
            {
              "id": "session-17",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "18:00",
          "endtime": "18:50",
          "sessions": [
            {
              "id": "session-18",
              "session_title": "Invited Talk",
              "talks": [
                {
                  "id": "talk-13",
                  "title": "What can we learn from cryptography to develop more trustworthy ML? ",
                  "authors": [
                    "Nicolas Papernot"
                  ],
                  "affiliations": "University of Toronto"
                }
              ]
            }
          ]
        },
        {
          "starttime": "18:50",
          "endtime": "19:00",
          "sessions": [
            {
              "id": "session-19",
              "session_title": "Break"
            }
          ]
        },
        {
          "starttime": "19:00",
          "endtime": "20:00",
          "sessions": [
            {
              "id": "session-20",
              "session_title": "Contributed Talks",
              "talks": [
                {
                  "paperId": 117,
                  "title": "NeuraCrypt is not private",
                  "affiliations": "Google; UC Berkeley and NTT Research; University of Wisconsin; Princeton University; University of Virginia; Stanford University and Google",
                  "abstract": "NeuraCrypt (Yara et al. arXiv 2021) is an algorithm that converts a sensitive dataset to\r\nan encoded dataset so that\r\n(1) it is still possible to train machine learning models on\r\nthe encoded data, but\r\n(2) an adversary who has access only to the encoded dataset can\r\nnot learn much about the original sensitive dataset.\r\nWe break NeuraCrypt's privacy claims, by perfectly solving the authors' public challenge, and by showing that NeuraCrypt does not satisfy\r\nthe formal privacy definitions posed in the original paper.\r\nOur attack consists of a series of boosting steps that,\r\ncoupled with various design flaws,\r\nturns a 1% attack advantage into a 100% complete break of the scheme.",
                  "authors": [
                    "Nicholas Carlini",
                    "Sanjam Garg",
                    "Somesh Jha",
                    "Saeed Mahloujifar",
                    "Mohammad Mahmoody",
                    "Florian Tramer"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-31"
                },
                {
                  "paperId": 114,
                  "title": "Secure Poisson Regression",
                  "affiliations": "Cornell Tech, Cornell University; George Mason University; Google",
                  "abstract": "We introduce the first construction for secure two-party computation of Poisson regression, which enables two parties who hold shares of the input samples to learn only the resulting Poisson model while protecting the privacy of the inputs.\r\n\r\nOur construction relies on new protocols for secure fixed-point exponentiation and correlated matrix multiplications. Our secure exponentiation construction avoids expensive bit decomposition and achieves orders of magnitude improvement in both online and offline costs over state of the art works. As a result, the dominant cost for our secure Poisson regression are matrix multiplications with one fixed matrix. We introduce a new technique, called correlated Beaver triples, which enables many such multiplications at the cost of roughly one matrix multiplication. This further brings down the cost of secure Poisson regression.\r\n\r\nWe implement our constructions and show their extreme efficiency. Our secure exponentiation for 20-bit fractional precision takes less than 0.07ms. One iteration of Poisson regression on a dataset with 10,000 samples with 1000 binary features, requires 9.91s offline time, 23.73s online computation and 7.28MB communication. For several real datasets this translates into training that takes seconds and only a couple of MB communication.",
                  "authors": [
                    "Mahimna Kelkar",
                    "Phi Hung Le",
                    "Mariana Raykova",
                    "Karn Seth"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-28"
                },
                {
                  "paperId": 104,
                  "title": "MUSE: Secure Inference Resilient to Malicious Clients",
                  "affiliations": "UC Berkeley; Tata Institute of Fundamental Research",
                  "abstract": "The increasing adoption of machine learning inference in applications has led to a corresponding increase in concerns surrounding the privacy guarantees offered by existing mechanisms for inference. Such concerns have motivated the construction of efficient secure inference protocols that allow parties to perform inference without revealing their sensitive information. Recently, there has been a proliferation of such proposals, rapidly improving efficiency. However, most of these protocols assume that the client is semi-honest, that is, the client does not deviate from the protocol; yet in practice, clients are many, have varying incentives, and can behave arbitrarily.\r\n\r\nTo demonstrate that a malicious client can completely break the security of semi-honest protocols, we first develop a new model-extraction attack against many state-of-the-art secure inference protocols. Our attack enables a malicious client to learn model weights with 22×-312× fewer queries than the best black-box model-extraction attack and scales to much deeper networks.\r\n\r\nMotivated by the severity of our attack, we design and implement MUSE, an efficient two-party secure inference protocol resilient to malicious clients. MUSE introduces a novel cryptographic protocol for conditional disclosure of secrets to switch between authenticated additive secret shares and garbled circuit labels, and an improved Beaver's triple generation procedure which is 8×-12.5× faster than existing techniques.\r\n\r\nThese protocols allow MUSE to push a majority of its cryptographic overhead into a preprocessing phase: compared to the equivalent semi-honest protocol (which is close to state-of-the-art), MUSE's online phase is only 1.7×-2.2× slower and uses 1.4× more communication. Overall, MUSE is 13.4×-21× faster and uses 2×-3.6× less communication than existing secure inference protocols which defend against malicious clients.",
                  "authors": [
                    "Ryan Lehmkuhl",
                    "Pratyush Mishra",
                    "Akshayaram Srinivasan",
                    "Raluca Ada Popa"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-24"
                },
                {
                  "paperId": 101,
                  "title": "Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning",
                  "affiliations": "CMU; MIT; UC Berkeley; NYU",
                  "abstract": "Many organizations need large amounts of high-quality data for their applications, and one way to acquire such data is via combining datasets from multiple parties. Since these organizations often own sensitive data that cannot be shared in the clear with others due to policy regulation and business competition, there is increased interest in utilizing secure multi-party computation (MPC). MPC allows multiple parties to jointly compute a function without revealing their inputs to each other.\r\n\r\nWe present Cerebro, an end-to-end collaborative learning platform that enables parties to compute learning tasks without sharing plaintext data. By taking an end-to-end approach to the system design, Cerebro allows multiple parties with complex economic relationships to safely collaborate on machine learning computation through the use of release policies and auditing, while also enabling users to achieve good performance without manually navigating the complex performance tradeoffs between MPC protocols.\r\n\r\n********************\r\n\r\nThe paper will appear in USENIX Security 2021. \r\n\r\nThe talk contributed to PPML 2021 will focus on some under-explored research questions in secure multiparty computation, such as (1) efficient auditing via commitments of input, (2) the need for MPC in asymmetric network conditions, and (3) achieving efficiency for concrete learning tasks.",
                  "authors": [
                    "Wenting Zheng",
                    "Ryan Deng",
                    "Weikeng Chen",
                    "Raluca Ada Popa",
                    "Aurojit Panda",
                    "Ion Stoica"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-21"
                },
                {
                  "paperId": 116,
                  "title": "Privacy-preserving machine learning for support vector machines",
                  "affiliations": "Universidad Nacional de Colombia; Aarhus University",
                  "abstract": "The recent availability of data and the increase of computational power makes machine learning an attractive tool for scientists and engineers. \r\nHowever, the increasing amount of available information demands security measures for protecting for sensitive data. To achieve this goal, multi-party computation techniques can be used, which enable a group of parties to jointly compute a given function while keeping the information provided by each party hidden. In this work, we study how to use multi-party computation in order to train support vector machines from both a theoretical and a practical perspective, including a complexity analysis of different algorithms that can be used for this goal, together with a full-fledged implementation in the MP-SPDZ framework for MPC.\r\n\r\nOur results show that multi-party computation is a viable solution for the task of training SVMs securely, obtaining considerable efficiency and little-to-none degradation of accuracy with respect to a cleartext implementation, although different considerations like dataset dimension and network quality must be taken into account for the applicability of these techniques in different contexts. In this work we discuss these considerations, along with some other practicalities necessary for the applicability of these techniques in realistic scenarios.",
                  "authors": [
                    "Daniel Cabarcas",
                    "Hernán D. Vanegas",
                    "Daniel E. Escudero"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-30"
                },
                {
                  "paperId": 115,
                  "title": "Improved Multi-Party Fixed-Point Multiplication",
                  "affiliations": "Visa Research; Northeastern University; University of Illinois at Chicago",
                  "abstract": "Machine learning is widely used for a range of applications and is increasingly offered as a service by major technology companies. However, the required massive data collection raises privacy concerns during both training and inference. Privacy-preserving machine learning aims to solve this problem. In this setting, a collection of servers secret share their data and use secure multi-party computation to train and evaluate models on the joint data. All prior work focussed on the scenario where the number of servers is two or three. In this work, we study the problem where there are N >= 3 servers amongst whom the data is secret shared. \r\n\r\nA key component of machine learning algorithms is to perform fixed-point multiplication with truncation of secret shared decimal values. In this work, we design new protocols for multi-party secure fixed-point multiplication where each of the N parties have one share each of the two values to be multiplied and receive one share of the product at the end of the protocol. We consider three forms of secret sharing - replicated, Shamir, and additive, and design an efficient protocol secure in the presence of a semi-honest adversary for each of the forms. Our protocols are more communication efficient than all prior work on performing multi-party fixed-point multiplication. Additionally, for replicated secret sharing, we design another efficient protocol that is secure in the presence of a malicious adversary. Finally, we leverage our fixed-point multiplication protocols to design secure multi-party computation (MPC) protocols for arbitrary arithmetic circuits that have addition and fixed-point multiplication with truncation gates. All our protocols are proven secure using a standard simulation based security definition. Our protocols for replicated and Shamir sharing work in the presence of an honest majority of parties while the one for additive sharing can tolerate a dishonest majority as well.",
                  "authors": [
                    "Saikrishna Badrinarayanan",
                    "Eysa Lee",
                    "Peihan Miao",
                    "Peter Rindal"
                  ],
                  "category": "Uncategorized",
                  "id": "talk-29"
                }
              ]
            }
          ]
        }
      ]
    }
  ],
  "database_id": "440"
}